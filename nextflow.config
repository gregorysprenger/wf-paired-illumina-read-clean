// Load base.config by default for all pipelines
// For big data jobs, use big data config instead
includeConfig (params.bigdata ? "conf/big_data.config" : "conf/base.config")

profiles {

    docker {
        docker.enabled         = true
        docker.userEmulation   = true
        fixOwnership           = true
        runOptions             = "-u \$(id -u):\$(id -g)"
        singularity.enabled    = false
        shifter.enabled        = false
    }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        shifter.enabled        = false
    }

    sge {
        process {
            executor = 'sge'
            penv     = 'smp'
            queue    = 'all.q'
        }
    }
    highmem {
      process {
        queue = 'highmem.q'
      }
    }
    // Test profiles
    test    { includeConfig 'conf/test.config' }
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')

// Global default params
params {

    // Input options
    inpath                      = null

    // Boilerplate options
    bigdata                    = false
    outpath                    = new File("${launchDir}").getCanonicalPath()
    logpath                    = new File("${params.outpath}/log").getCanonicalPath()
    process_log_dir            = new File("${params.outpath}/log/process_logs").getCanonicalPath()
    publish_dir_mode           = 'copy'
    help = false
    version = false

    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '128.GB'
    max_cpus                   = 16
    max_time                   = '240.h'
}


timeline {
    enabled = true
    file = "${params.logpath}/timeline_${trace_timestamp}.html"
}

report {
    enabled = true
    file = "${params.logpath}/report_${trace_timestamp}.html"
}

trace {
    enabled = true
    fields = 'task_id,name,status,exit,realtime,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar'
    file = "${params.logpath}/trace_${trace_timestamp}.txt"
}

dag {
    enabled = true
    file = "${params.logpath}/pipeline_dag_${trace_timestamp}.html"
}

manifest {
    name = 'wf-paired-illumina-read-clean'
    author = 'Christopher A. Gulvik'
    description = 'Trim, assemble, and annotate paired end illumina reads'
    mainScript = 'main.nf'
    nextflowVersion = '>=20.01.0'
    version = '1.0.0'
}

// Function to ensure that resource requirements don't go beyond a maximum limit
// This code is from: https://github.com/nf-core/rnaseq/blob/3643a94411b65f42bce5357c5015603099556ad9/nextflow.config
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}